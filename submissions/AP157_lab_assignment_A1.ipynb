{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"RSdzzrWrgAXu"},"source":["# **AP157 Machine Learning Lab Assignment**\n","## Module A1: Regression -- Overfitting, Underfitting, and Cross-Validation\n","\n","_Instructions_: Answer each part of the assignment as completely as you can. Discuss **all** your code and results as clearly and concisely as possible.\n","\n","_Scoring Criteria_: 50% - *correctness of code*; 50% - *discussion of the code and results*. Maximum score is **100 points** (Parts 1 and 2 are worth 20 and 80 points, respectively).\n","\n","_Credits_: This assignment is based on Chapter 8.11 of “Statistics, Data Mining, and Machine Learning in Astronomy” (SDMMLA) by Zeljko Ivezic et al."]},{"cell_type":"markdown","metadata":{"id":"I_dmUyo2pxIt"},"source":["### Student Information\n","\n","_Full Name (Last Name, First Name)_: \\\n","_Student No._:\\\n","_Section_:"]},{"cell_type":"markdown","metadata":{"id":"LyO1CnkFp17G"},"source":["### Submission Information\n","\n","_Date and Time Submitted (most recent upload)_:\n","\n","**HONOR PLEDGE** I affirm that I have upheld the highest principles of honesty and integrity in my academic work and that this lab assignment is my own work.\n","\n","**Sign here with your full name:**"]},{"cell_type":"markdown","metadata":{"id":"xgse8Mt5p35S"},"source":["### Grading Information (c/o Instructor)\n","\n","TOTAL SCORE: **[]**/100\n","\n","Score breakdown:\n","* Part 1 - []/20\n","* Part 2 - []/80\n","\n","_Date and Time Scored (MM/DD/YYYY HH:MM AM/PM):_"]},{"cell_type":"code","metadata":{"id":"KAJ7b7cMp6FY"},"source":["import numpy as np\n","from matplotlib import pyplot as plt\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gAgom2vqp-2X"},"source":["#### PART 1 - Generate data set *(20 points)*\n","\n","1. Generate a toy data set with 100 regularly-spaced points and as described by Eq. 8.75-- a simple model where $x$ and $y$ satisfy the following:\n","$$ 0 \\le x_i \\le 3 $$\n","$$ y_i = x_i \\sin(x_i) + \\epsilon_i $$\n","where the noise is drawn from a normal distribution $\\epsilon_i \\sim \\rm{N}(0, 0.1)$. *(10 points)*  \n","\n","2. Plot your toy data set (Refer to Fig. 8.12 for comparison). *(10 points)*\n"]},{"cell_type":"markdown","metadata":{"id":"YbKhGKmgsPCV"},"source":["PART 2 - Apply cross-validation *(80 points)*\n","\n","Recreate the top panel of Figure 8.14: rms error vs. polynomial degree for the training set and cross-validation set. To do this, you will perform the ff. steps:\n","\n","1. Split the data set into training, cross-validation, and test sets with 50%, 25% and 25% ratio. You can use sklearn library's model_selection.train_test_split function [(link)](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html). *(20 pts.)*\n","2. Get the best-fit curve for polynomial degrees $d=0$ to 14 for the training set. You can use numpy library's polyfit function [(link)](https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html). *(30 pts.)*\n","3. Get the rms errors for both the training and cross-validation sets (for the best-fit curve from Step 2). *(10 pts.)*\n","4. Plot rms errors for both the training and cross-validation sets against polynomial degree $d$ (as in Fig. 8.14). *(20 pts.)*\n"]},{"cell_type":"code","metadata":{"id":"y8_C0CpvqlkI"},"source":[],"execution_count":null,"outputs":[]}]}